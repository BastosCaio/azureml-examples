{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data\n",
    "\n",
    "In this notebook you will learn how to use the AzureML SDK to:\n",
    "\n",
    "1. Read/write data in a job.\n",
    "1. Create a data asset to share with others in your team.\n",
    "1. Abstract schema for tabular data using `MLTable`.\n",
    "\n",
    "## Connect to Azure Machine Learning Workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# enter details of your AML workspace\n",
    "subscription_id = \"09addf29-d567-4494-9d05-db2491ceff53\"\n",
    "resource_group = \"AZUSDSPROJ\"\n",
    "workspace = \"azureml6318355454\"\n",
    "\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading/writing data in a job\n",
    "\n",
    "In this example we will use the titanic dataset in this repo - ([./sample_data/titanic.csv](./sample_data/titanic.csv)) and set-up a command that executes the following python code:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(args.input_data)\n",
    "print(df.head(10))\n",
    "```\n",
    "\n",
    "Below is the code for submitting the command to the cloud - note that both the code *and* the data is automatically uploaded to the cloud. Note: The data is only re-uploaded on subsequent job submissions if data has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Exception in thread Thread-6 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Caio\\anaconda3\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Caio\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\Caio\\anaconda3\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Caio\\anaconda3\\Lib\\subprocess.py\", line 1597, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "  File \"<frozen codecs>\", line 322, in decode\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc6 in position 8: invalid continuation byte\n",
      "DefaultAzureCredential failed to retrieve a token from the included credentials.\n",
      "Attempted credentials:\n",
      "\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\n",
      "Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n",
      "\tManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.\n",
      "\tSharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\n",
      "\tAzureCliCredential: Azure CLI not found on path\n",
      "\tAzurePowerShellCredential: Failed to invoke PowerShell.\n",
      "To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/powershellcredential/troubleshoot.\n",
      "\tAzureDeveloperCliCredential: Azure Developer CLI could not be found. Please visit https://aka.ms/azure-dev for installation instructions and then,once installed, authenticate to your Azure account using 'azd auth login'.\n",
      "To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\n"
     ]
    },
    {
     "ename": "ClientAuthenticationError",
     "evalue": "DefaultAzureCredential failed to retrieve a token from the included credentials.\nAttempted credentials:\n\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\nVisit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n\tManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.\n\tSharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\n\tAzureCliCredential: Azure CLI not found on path\n\tAzurePowerShellCredential: Failed to invoke PowerShell.\nTo mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/powershellcredential/troubleshoot.\n\tAzureDeveloperCliCredential: Azure Developer CLI could not be found. Please visit https://aka.ms/azure-dev for installation instructions and then,once installed, authenticate to your Azure account using 'azd auth login'.\nTo mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientAuthenticationError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m\n\u001b[0;32m     15\u001b[0m job \u001b[38;5;241m=\u001b[39m command(\n\u001b[0;32m     16\u001b[0m     code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./src\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# local path where the code is stored\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     command\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython read_data.py --input_data $\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124minputs.input_data}}\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     compute\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu-cluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# submit the command\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m returned_job \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# get a URL for the status of the job\u001b[39;00m\n\u001b[0;32m     26\u001b[0m returned_job\u001b[38;5;241m.\u001b[39mstudio_url\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:350\u001b[0m, in \u001b[0;36mmonitor_with_telemetry_mixin.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameter_dimensions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(custom_dimensions \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, dimensions) \u001b[38;5;28;01mas\u001b[39;00m activityLogger:\n\u001b[1;32m--> 350\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parameter_dimensions:\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;66;03m# collect from return if no dimensions from parameter\u001b[39;00m\n\u001b[0;32m    353\u001b[0m         activityLogger\u001b[38;5;241m.\u001b[39mactivity_info\u001b[38;5;241m.\u001b[39mupdate(_collect_from_return_value(return_value))\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:647\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[1;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate(job, raise_on_failure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;66;03m# Create all dependent resources\u001b[39;00m\n\u001b[1;32m--> 647\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_arm_id_or_upload_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ValidationException, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m ex:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    649\u001b[0m     log_and_raise_error(ex)\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:1049\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_or_upload_dependencies\u001b[1;34m(self, job)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_resolve_arm_id_or_upload_dependencies\u001b[39m(\u001b[38;5;28mself\u001b[39m, job: Job) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This method converts name or name:version to ARM id. Or it\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m    registers/uploads nested dependencies.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;124;03m    :rtype: Job\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1049\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_arm_id_or_azureml_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_orchestrators\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_asset_arm_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job, PipelineJob):\n\u001b[0;32m   1052\u001b[0m         \u001b[38;5;66;03m# Resolve top-level inputs\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_job_inputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_group_inputs(job\u001b[38;5;241m.\u001b[39minputs), job\u001b[38;5;241m.\u001b[39m_base_path)\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:1294\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_or_azureml_id\u001b[1;34m(self, job, resolver)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     job\u001b[38;5;241m.\u001b[39mcompute \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_compute_id(resolver, job\u001b[38;5;241m.\u001b[39mcompute)\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job, Command):\n\u001b[1;32m-> 1294\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_arm_id_for_command_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job, ImportJob):\n\u001b[0;32m   1296\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_arm_id_for_import_job(job, resolver)\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:1340\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_for_command_job\u001b[1;34m(self, job, resolver)\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationException(\n\u001b[0;32m   1332\u001b[0m         message\u001b[38;5;241m=\u001b[39mmsg\u001b[38;5;241m.\u001b[39mformat(job\u001b[38;5;241m.\u001b[39mcode),\n\u001b[0;32m   1333\u001b[0m         target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1336\u001b[0m         error_type\u001b[38;5;241m=\u001b[39mValidationErrorType\u001b[38;5;241m.\u001b[39mINVALID_VALUE,\n\u001b[0;32m   1337\u001b[0m     )\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_ARM_id_for_resource(job\u001b[38;5;241m.\u001b[39mcode, AzureMLResourceType\u001b[38;5;241m.\u001b[39mCODE):\n\u001b[1;32m-> 1340\u001b[0m     job\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m=\u001b[39m \u001b[43mresolver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mazureml_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAzureMLResourceType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1344\u001b[0m job\u001b[38;5;241m.\u001b[39menvironment \u001b[38;5;241m=\u001b[39m resolver(job\u001b[38;5;241m.\u001b[39menvironment, azureml_type\u001b[38;5;241m=\u001b[39mAzureMLResourceType\u001b[38;5;241m.\u001b[39mENVIRONMENT)\n\u001b[0;32m   1345\u001b[0m job\u001b[38;5;241m.\u001b[39mcompute \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_compute_id(resolver, job\u001b[38;5;241m.\u001b[39mcompute)\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_operation_orchestrator.py:232\u001b[0m, in \u001b[0;36mOperationOrchestrator.get_asset_arm_id\u001b[1;34m(self, asset, azureml_type, register_asset, sub_workspace_resource)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m# TODO: once the asset redesign is finished, this logic can be replaced with unified API\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m azureml_type \u001b[38;5;241m==\u001b[39m AzureMLResourceType\u001b[38;5;241m.\u001b[39mCODE \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(asset, Code):\n\u001b[1;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_code_asset_arm_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43masset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregister_asset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregister_asset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m azureml_type \u001b[38;5;241m==\u001b[39m AzureMLResourceType\u001b[38;5;241m.\u001b[39mENVIRONMENT \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(asset, Environment):\n\u001b[0;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_environment_arm_id(asset, register_asset\u001b[38;5;241m=\u001b[39mregister_asset)\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_operation_orchestrator.py:298\u001b[0m, in \u001b[0;36mOperationOrchestrator._get_code_asset_arm_id\u001b[1;34m(self, code_asset, register_asset)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uploaded_code_asset\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (MlException, HttpResponseError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AssetException(\n\u001b[0;32m    301\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError with code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    302\u001b[0m         target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mASSET,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mSYSTEM_ERROR,\n\u001b[0;32m    306\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_operation_orchestrator.py:273\u001b[0m, in \u001b[0;36mOperationOrchestrator._get_code_asset_arm_id\u001b[1;34m(self, code_asset, register_asset)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_datastore_name(code_asset\u001b[38;5;241m.\u001b[39mpath)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m register_asset:\n\u001b[1;32m--> 273\u001b[0m     code_asset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_code_assets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_asset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m code_asset\u001b[38;5;241m.\u001b[39mid\n\u001b[0;32m    275\u001b[0m sas_info \u001b[38;5;241m=\u001b[39m get_storage_info_for_non_registry_asset(\n\u001b[0;32m    276\u001b[0m     service_client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code_assets\u001b[38;5;241m.\u001b[39m_service_client,\n\u001b[0;32m    277\u001b[0m     workspace_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operation_scope\u001b[38;5;241m.\u001b[39mworkspace_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    280\u001b[0m     resource_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operation_scope\u001b[38;5;241m.\u001b[39mresource_group_name,\n\u001b[0;32m    281\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:275\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[1;32m--> 275\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_code_operations.py:212\u001b[0m, in \u001b[0;36mCodeOperations.create_or_update\u001b[1;34m(self, code)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ex) \u001b[38;5;241m==\u001b[39m ASSET_PATH_ERROR:\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AssetPathException(\n\u001b[0;32m    207\u001b[0m             message\u001b[38;5;241m=\u001b[39mCHANGED_ASSET_PATH_MSG,\n\u001b[0;32m    208\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mCODE,\n\u001b[0;32m    209\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39mCHANGED_ASSET_PATH_MSG_NO_PERSONAL_DATA,\n\u001b[0;32m    210\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[0;32m    211\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_code_operations.py:136\u001b[0m, in \u001b[0;36mCodeOperations.create_or_update\u001b[1;34m(self, code)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m snapshot_path_info:\n\u001b[0;32m    135\u001b[0m     _, _, asset_hash \u001b[38;5;241m=\u001b[39m snapshot_path_info\n\u001b[1;32m--> 136\u001b[0m     existing_assets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_version_operation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresource_group_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resource_group_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masset_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhash_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mget_content_hash_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(existing_assets) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    147\u001b[0m         existing_asset \u001b[38;5;241m=\u001b[39m existing_assets[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\core\\paging.py:123\u001b[0m, in \u001b[0;36mItemPaged.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_iterator \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mby_page())\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_page_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\core\\paging.py:75\u001b[0m, in \u001b[0;36mPageIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd of paging\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_next\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinuation_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AzureError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m error\u001b[38;5;241m.\u001b[39mcontinuation_token:\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\_restclient\\v2023_04_01\\operations\\_code_versions_operations.py:378\u001b[0m, in \u001b[0;36mCodeVersionsOperations.list.<locals>.get_next\u001b[1;34m(next_link)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_next\u001b[39m(next_link\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    376\u001b[0m     request \u001b[38;5;241m=\u001b[39m prepare_request(next_link)\n\u001b[1;32m--> 378\u001b[0m     pipeline_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m     response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:230\u001b[0m, in \u001b[0;36mPipeline.run\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m pipeline_request: PipelineRequest[HTTPRequestType] \u001b[38;5;241m=\u001b[39m PipelineRequest(request, context)\n\u001b[0;32m    229\u001b[0m first_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies \u001b[38;5;28;01melse\u001b[39;00m _TransportRunner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport)\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirst_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_request\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:86\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     84\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:86\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     84\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "    \u001b[1;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 86 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:86\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     84\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\mgmt\\core\\policies\\_base.py:46\u001b[0m, in \u001b[0;36mARMAutoResourceProviderRegistrationPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# type: (PipelineRequest[HTTPRequestType], Any) -> PipelineResponse[HTTPRequestType, HTTPResponseType]\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     http_request \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mhttp_request\n\u001b[1;32m---> 46\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mhttp_response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[0;32m     48\u001b[0m         rp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_rp_not_registered_err(response)\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\core\\pipeline\\policies\\_redirect.py:197\u001b[0m, in \u001b[0;36mRedirectPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    195\u001b[0m original_domain \u001b[38;5;241m=\u001b[39m get_domain(request\u001b[38;5;241m.\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl) \u001b[38;5;28;01mif\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retryable:\n\u001b[1;32m--> 197\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     redirect_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_redirect_location(response)\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m redirect_location \u001b[38;5;129;01mand\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\core\\pipeline\\policies\\_retry.py:531\u001b[0m, in \u001b[0;36mRetryPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_timeout(request, absolute_timeout, is_response_error)\n\u001b[1;32m--> 531\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_retry(retry_settings, response):\n\u001b[0;32m    533\u001b[0m         retry_active \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincrement(retry_settings, response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\core\\pipeline\\policies\\_authentication.py:124\u001b[0m, in \u001b[0;36mBearerTokenCredentialPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: PipelineRequest[HTTPRequestType]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PipelineResponse[HTTPRequestType, HTTPResponseType]:\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Authorize request with a bearer token and send it to the next policy\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    :param request: The pipeline request object\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m    :rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    126\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\core\\pipeline\\policies\\_authentication.py:99\u001b[0m, in \u001b[0;36mBearerTokenCredentialPolicy.on_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credential\u001b[38;5;241m.\u001b[39mget_token(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scopes, enable_cae\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_cae)\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_credential\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scopes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_headers(request\u001b[38;5;241m.\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token\u001b[38;5;241m.\u001b[39mtoken)\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\identity\\_credentials\\default.py:225\u001b[0m, in \u001b[0;36mDefaultAzureCredential.get_token\u001b[1;34m(self, claims, tenant_id, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token\n\u001b[0;32m    224\u001b[0m within_dac\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 225\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclaims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclaims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtenant_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m within_dac\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "File \u001b[1;32mc:\\Users\\Caio\\anaconda3\\Lib\\site-packages\\azure\\identity\\_credentials\\chained.py:124\u001b[0m, in \u001b[0;36mChainedTokenCredential.get_token\u001b[1;34m(self, claims, tenant_id, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m failed to retrieve a token from the included credentials.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m )\n\u001b[0;32m    123\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientAuthenticationError(message\u001b[38;5;241m=\u001b[39mmessage)\n",
      "\u001b[1;31mClientAuthenticationError\u001b[0m: DefaultAzureCredential failed to retrieve a token from the included credentials.\nAttempted credentials:\n\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\nVisit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n\tManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.\n\tSharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\n\tAzureCliCredential: Azure CLI not found on path\n\tAzurePowerShellCredential: Failed to invoke PowerShell.\nTo mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/powershellcredential/troubleshoot.\n\tAzureDeveloperCliCredential: Azure Developer CLI could not be found. Please visit https://aka.ms/azure-dev for installation instructions and then,once installed, authenticate to your Azure account using 'azd auth login'.\nTo mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot."
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# === Note on path ===\n",
    "# can be can be a local path or a cloud path. AzureML supports https://`, `abfss://`, `wasbs://` and `azureml://` URIs.\n",
    "# Local paths are automatically uploaded to the default datastore in the cloud.\n",
    "# More details on supported paths: https://docs.microsoft.com/azure/machine-learning/how-to-read-write-data-v2#supported-paths\n",
    "\n",
    "inputs = {\n",
    "    \"input_data\": Input(type=AssetTypes.URI_FILE, path=\"./sample_data/titanic.csv\")\n",
    "}\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",  # local path where the code is stored\n",
    "    command=\"python read_data.py --input_data ${{inputs.input_data}}\",\n",
    "    inputs=inputs,\n",
    "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n",
    "    compute=\"cpu-cluster\",\n",
    ")\n",
    "\n",
    "# submit the command\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "# get a URL for the status of the job\n",
    "returned_job.studio_url"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading *and* writing data in a job\n",
    "\n",
    "By design, you cannot *write* to `Inputs` only `Outputs`. The code below creates an `Output` that will mount your AzureML default datastore (Azure Blob) in Read-*Write* mode. The python code simply takes the CSV as import and exports it as a parquet file, i.e.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(args.input_data)\n",
    "output_path = os.path.join(args.output_folder, \"my_output.parquet\")\n",
    "df.to_parquet(output_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "inputs = {\n",
    "    \"input_data\": Input(type=AssetTypes.URI_FILE, path=\"./sample_data/titanic.csv\")\n",
    "}\n",
    "\n",
    "outputs = {\n",
    "    \"output_folder\": Output(\n",
    "        type=AssetTypes.URI_FOLDER,\n",
    "        path=f\"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/workspaceblobstore/paths/\",\n",
    "    )\n",
    "}\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",  # local path where the code is stored\n",
    "    command=\"python read_write_data.py --input_data ${{inputs.input_data}} --output_folder ${{outputs.output_folder}}\",\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n",
    "    compute=\"cpu-cluster\",\n",
    ")\n",
    "\n",
    "# submit the command\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "# get a URL for the status of the job\n",
    "returned_job.studio_url"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Assets\n",
    "\n",
    "You can create a data asset in Azure Machine Learning, which has the following benefits:\n",
    "\n",
    "- Easy to share with other members of the team (no need to remember file locations)\n",
    "- Versioning of the metadata (location, description, etc)\n",
    "- Lineage tracking\n",
    "\n",
    "Below we show an example of versioning the sample data in this repo. The data is uploaded to cloud storage and registered as an asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "try:\n",
    "    registered_data_asset = ml_client.data.get(name=\"titanic\", version=\"1\")\n",
    "    print(\"Found data asset. Will not create again\")\n",
    "except Exception as ex:\n",
    "    my_data = Data(\n",
    "        path=\"./sample_data/titanic.csv\",\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        description=\"Titanic Data\",\n",
    "        name=\"titanic\",\n",
    "        version=\"1\",\n",
    "    )\n",
    "    ml_client.data.create_or_update(my_data)\n",
    "    registered_data_asset = ml_client.data.get(name=\"titanic\", version=\"1\")\n",
    "    print(\"Created data asset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate with user identity\n",
    "\n",
    "When running a job on a compute cluster, you can also use your user identity to access data. To enable job to access data on behald of you, specify **identity=UserIdentity()** in job definition, as shown below. For more details, see [Accessing storage services](https://learn.microsoft.com/azure/machine-learning/how-to-identity-based-service-authentication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import UserIdentityConfiguration\n",
    "\n",
    "my_job_inputs = {\"input_data\": Input(type=AssetTypes.MLTABLE, path=\"./sample_data\")}\n",
    "job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"python read_data.py --input_data ${{inputs.input_data}}\",\n",
    "    inputs=my_job_inputs,\n",
    "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n",
    "    compute=\"cpu-cluster\",\n",
    "    identity=UserIdentityConfiguration(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLTable\n",
    "\n",
    "`MLTable` is a way to abstract the schema definition for tabular data so that it is easier for consumers of the data to materialize the table into a Pandas/Dask/Spark dataframe. [A more detailed explanation and motivation is provided on docs.microsoft.com.](https://docs.microsoft.com/azure/machine-learning/concept-data#mltable).\n",
    "\n",
    "The ideal scenarios to use `MLTable` are:\n",
    "\n",
    "- The schema of your data is complex and/or changes frequently.\n",
    "- You only need a subset of data (for example: a sample of rows or files, specific columns, etc).\n",
    "- AutoML jobs requiring tabular data.\n",
    "\n",
    "If your scenario does not fit the above then it is likely that URIs are a more suitable type.\n",
    "\n",
    "### The `MLTable` file\n",
    "\n",
    "The `MLTable` file defines the schema for tabular data. Below is a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./sample-mltable/MLTable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend that you co-locate your `MLTable` file with the underlying data (i.e. the `MLTable` file should be in the same (or parent) directory. You can can load an `MLTable` artefact using the `mltable` library - below below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mltable\n",
    "\n",
    "# Note: the uri below can be a local folder or folder located in cloud storage. The folder must contain a valid MLTable file.\n",
    "tbl = mltable.load(uri=\"./sample-mltable\")\n",
    "tbl.to_pandas_dataframe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data from external sources\n",
    "\n",
    "You can create a data asset in Azure Machine Learning by importing data from external sources like Snowflake or Amazon S3 and caching it, which has the following benefits:\n",
    "\n",
    "- All the benefits of caching including faster and more reliable access of data for the training jobs\n",
    "- Less points of failure than connecting to data in external sources directly\n",
    "- Easy to share with other members of the team (no need to remember file locations)\n",
    "- Versioning of the metadata (location, description, etc) even for data from SQL/ relational database \n",
    "- Lineage tracking\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, you need to create a workspace connection with details on how to connect to the external source. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a connection to Snowflake DB when you want to import data from Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import WorkspaceConnection\n",
    "from azure.ai.ml.entities import UsernamePasswordConfiguration\n",
    "\n",
    "name = \"my_snowflakedb_connection\"\n",
    "\n",
    "target = \"jdbc:snowflake://<myaccount>.snowflakecomputing.com/?db=<mydb>&warehouse=<mywarehouse>&role=<myrole>\"\n",
    "# add the Snowflake account, database, warehouse name and role name here. If no role name provided it will default to PUBLIC\n",
    "\n",
    "wps_connection = WorkspaceConnection(\n",
    "    name=name,\n",
    "    type=\"snowflake\",\n",
    "    target=target,\n",
    "    credentials=UsernamePasswordConfiguration(username=\"XXXXX\", password=\"XXXXXX\"),\n",
    ")\n",
    "ml_client.connections.create_or_update(workspace_connection=wps_connection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a connection to Azure SQL DB when you want to import data from Azure SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import WorkspaceConnection\n",
    "from azure.ai.ml.entities import UsernamePasswordConfiguration\n",
    "\n",
    "name = \"my_sqldb_connection\"\n",
    "target = \"Server=tcp:<myservername>,<port>;Database=<mydatabase>;Trusted_Connection=False;Encrypt=True;Connection Timeout=30\"\n",
    "# add the sql servername, port address and database\n",
    "\n",
    "wps_connection = WorkspaceConnection(\n",
    "    name=name,\n",
    "    type=\"azure_sql_db\",\n",
    "    target=target,\n",
    "    credentials=UsernamePasswordConfiguration(username=\"XXXXX\", password=\"XXXXXX\"),\n",
    ")\n",
    "ml_client.connections.create_or_update(workspace_connection=wps_connection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a connection to Amazon S3 when you want to import data from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import WorkspaceConnection\n",
    "from azure.ai.ml.entities import AccessKeyConfiguration\n",
    "\n",
    "name = \"my_s3_connection\"\n",
    "target = \"<mybucket>\"  # add the s3 bucket details\n",
    "wps_connection = WorkspaceConnection(\n",
    "    name=name,\n",
    "    type=\"s3\",\n",
    "    target=target,\n",
    "    credentials=AccessKeyConfiguration(\n",
    "        access_key_id=\"XXXXXX\", secret_access_key=\"XXXXXXXX\"\n",
    "    ),\n",
    ")\n",
    "ml_client.connections.create_or_update(workspace_connection=wps_connection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the connection is created, you need to initiate an Import job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of importing data from Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import DataImport\n",
    "from azure.ai.ml.data_transfer import Database\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Supported connections include:\n",
    "# Connection: azureml:<workspace_connection_name>\n",
    "# Supported paths include:\n",
    "# Datastore: azureml://datastores/<data_store_name>/paths/<my_path>/${{name}}\n",
    "\n",
    "data_import = DataImport(\n",
    "    name=\"snowflake_sample\",\n",
    "    source=Database(\n",
    "        connection=\"azureml:my_snowflakedb_connection\",\n",
    "        query=\"select * from my_sample_table\",\n",
    "    ),\n",
    "    path=\"azureml://datastores/workspaceblobstore/paths/snowflake/${{name}}\",\n",
    ")\n",
    "ml_client.data.import_data(data_import=data_import)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of importing data from Azure SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import DataImport\n",
    "from azure.ai.ml.data_transfer import Database\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Supported connections include:\n",
    "# Connection: azureml:<workspace_connection_name>\n",
    "# Supported paths include:\n",
    "# Datastore: azureml://datastores/<data_store_name>/paths/<my_path>/${{name}}\n",
    "\n",
    "data_import = DataImport(\n",
    "    name=\"azuresql_sample\",\n",
    "    source=Database(\n",
    "        connection=\"azureml:my_sqldb_connection\", query=\"select * from my_table\"\n",
    "    ),\n",
    "    path=\"azureml://datastores/workspaceblobstore/paths/azuresql/${{name}}\",\n",
    ")\n",
    "ml_client.data.import_data(data_import=data_import)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of importing data from Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import DataImport\n",
    "from azure.ai.ml.data_transfer import FileSystem\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Supported connections include:\n",
    "# Connection: azureml:<workspace_connection_name>\n",
    "# Supported paths include:\n",
    "# Datastore: azureml://datastores/<data_store_name>/paths/<my_path>/${{name}}\n",
    "\n",
    "data_import = DataImport(\n",
    "    name=\"s3_sample\",\n",
    "    source=FileSystem(\n",
    "        connection=\"azureml:my_s3_connection\", path=\"myfiles/titanic.csv\"\n",
    "    ),\n",
    "    path=\"azureml://datastores/workspaceblobstore/paths/s3/${{name}}\",\n",
    ")\n",
    "ml_client.data.import_data(data_import=data_import)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data on a Schedule. \n",
    "You can import data on a schedule created on a recurrence trigger or a cron trigger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of importing data from Snowflake on Recurrence trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.data_transfer import Database\n",
    "from azure.ai.ml.constants import TimeZone\n",
    "from azure.ai.ml.entities import (\n",
    "    ImportDataSchedule,\n",
    "    RecurrenceTrigger,\n",
    "    RecurrencePattern,\n",
    ")\n",
    "from datetime import datetime\n",
    "\n",
    "source = Database(connection=\"azureml:my_sf_connection\", query=\"select * from my_table\")\n",
    "\n",
    "path = \"azureml://datastores/workspaceblobstore/paths/snowflake/schedule/${{name}}\"\n",
    "\n",
    "\n",
    "my_data = DataImport(\n",
    "    type=\"mltable\", source=source, path=path, name=\"my_schedule_sfds_test\"\n",
    ")\n",
    "\n",
    "schedule_name = \"my_simple_sdk_create_schedule_recurrence\"\n",
    "\n",
    "schedule_start_time = datetime.utcnow()\n",
    "\n",
    "recurrence_trigger = RecurrenceTrigger(\n",
    "    frequency=\"day\",\n",
    "    interval=1,\n",
    "    schedule=RecurrencePattern(hours=1, minutes=[0, 1]),\n",
    "    start_time=schedule_start_time,\n",
    "    time_zone=TimeZone.UTC,\n",
    ")\n",
    "\n",
    "\n",
    "import_schedule = ImportDataSchedule(\n",
    "    name=schedule_name, trigger=recurrence_trigger, import_data=my_data\n",
    ")\n",
    "\n",
    "ml_client.schedules.begin_create_or_update(import_schedule).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a similar example of creating an import data on a schedule - this time it is a Cron Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import CronTrigger, ImportDataSchedule\n",
    "\n",
    "source = Database(connection=\"azureml:my_sf_connection\", query=\"select * from my_table\")\n",
    "\n",
    "path = \"azureml://datastores/workspaceblobstore/paths/snowflake/schedule/${{name}}\"\n",
    "\n",
    "\n",
    "my_data = DataImport(\n",
    "    type=\"mltable\", source=source, path=path, name=\"my_schedule_sfds_test\"\n",
    ")\n",
    "\n",
    "schedule_name = \"my_simple_sdk_create_schedule_cron\"\n",
    "\n",
    "cron_trigger = CronTrigger(\n",
    "    expression=\"15 10 * * 1\",\n",
    "    start_time=datetime.utcnow(),\n",
    "    end_time=\"2024-12-03T18:40:00\",\n",
    ")\n",
    "import_schedule = ImportDataSchedule(\n",
    "    name=schedule_name, trigger=cron_trigger, import_data=my_data\n",
    ")\n",
    "ml_client.schedules.begin_create_or_update(import_schedule).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The import schedule is a schedule, so all the other CRUD operations of Schedule are available on this as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable the schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.schedules.begin_disable(name=schedule_name).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the detail of the schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_schedule = ml_client.schedules.get(name=schedule_name)\n",
    "[created_schedule.name]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List schedules in a workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules = ml_client.schedules.list()\n",
    "[s.name for s in schedules]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable a schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.schedules.begin_enable(name=schedule_name).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update a schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update trigger expression\n",
    "import_schedule.trigger.expression = \"10 10 * * 1\"\n",
    "import_schedule = ml_client.schedules.begin_create_or_update(\n",
    "    schedule=import_schedule\n",
    ").result()\n",
    "print(import_schedule)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only disabled schedules can be deleted\n",
    "ml_client.schedules.begin_disable(name=schedule_name).result()\n",
    "ml_client.schedules.begin_delete(name=schedule_name).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Management on Workspace managed datastore:\n",
    "\n",
    "Data import can be performed to an AzureML managed HOBO storage called \"workspacemanageddatastore\" by specifying \n",
    "path: azureml://datastores/workspacemanageddatastore\n",
    "The datastore will be automatically back-filled if not present.\n",
    "\n",
    "When done so, it comes with the added benefit of data life cycle management.\n",
    "\n",
    "The following shows a simple data import on to the workspacemanageddatastore. Same can be done using the schedules defined above - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import DataImport\n",
    "from azure.ai.ml.data_transfer import Database\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Supported connections include:\n",
    "# Connection: azureml:<workspace_connection_name>\n",
    "# Supported paths include:\n",
    "# Datastore: azureml://datastores/<data_store_name>/paths/<my_path>/${{name}}\n",
    "\n",
    "data_import = DataImport(\n",
    "    name=\"my_sf_managedasset\",\n",
    "    source=Database(\n",
    "        connection=\"azureml:my_snowflakedb_connection\",\n",
    "        query=\"select * from my_sample_table\",\n",
    "    ),\n",
    "    path=\"azureml://datastores/workspacemanageddatastore\",\n",
    ")\n",
    "ml_client.data.import_data(data_import=data_import)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the examples of doing data lifecycle management aka altering the auto_delete_settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get imported data asset details:\n",
    "```python\n",
    "\n",
    "\n",
    "# Get data asset details\n",
    "name = \"my_sf_managedasset\"\n",
    "version = \"1\"\n",
    "my_data = ml_client.data.get(name=name, version=version)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update auto delete settings - "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.entities._assets.auto_delete_setting import AutoDeleteSetting\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# update auto delete setting\n",
    "name = \"my_sf_managedasset\"\n",
    "version = \"1\"\n",
    "my_data = ml_client.data.get(name=name, version=version)\n",
    "my_data.auto_delete_setting = AutoDeleteSetting(\n",
    "    condition=\"created_greater_than\", value=\"45d\"\n",
    ")\n",
    "my_data = ml_client.data.create_or_update(my_data)\n",
    "print(\"Update auto delete setting:\", my_data)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update auto delete settings - "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.entities._assets.auto_delete_setting import AutoDeleteSetting\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# update auto delete setting\n",
    "name = \"my_sf_managedasset\"\n",
    "version = \"1\"\n",
    "my_data = ml_client.data.get(name=name, version=version)\n",
    "my_data.auto_delete_setting = AutoDeleteSetting(\n",
    "    condition=\"last_accessed_greater_than\", value=\"30d\"\n",
    ")\n",
    "my_data = ml_client.data.create_or_update(my_data)\n",
    "print(\"Update auto delete setting:\", my_data)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete auto delete settings - "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.entities._assets.auto_delete_setting import AutoDeleteSetting\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# remove auto delete setting\n",
    "name = \"my_sf_managedasset\"\n",
    "version = \"1\"\n",
    "my_data = ml_client.data.get(name=name, version=version)\n",
    "my_data.auto_delete_setting = None\n",
    "my_data = ml_client.data.create_or_update(my_data)\n",
    "print(\"Remove auto delete setting:\", my_data)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Whilst the above example shows a local file. Remember that `path` supports cloud storage (`https`, `abfss`, `wasbs` protocols). Therefore, if you want to register data in a cloud location just specify the path with any of the supported protocols.\n",
    "\n",
    "### Consume data assets in a job\n",
    "\n",
    "Below shows how to consume a data asset in the job:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "registered_data_asset = ml_client.data.get(name=\"titanic\", version=\"1\")\n",
    "\n",
    "my_job_inputs = {\n",
    "    \"input_data\": Input(type=AssetTypes.URI_FILE, path=registered_data_asset.id)\n",
    "}\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"python read_data.py --input_data ${{inputs.input_data}}\",\n",
    "    inputs=my_job_inputs,\n",
    "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n",
    "    compute=\"cpu-cluster\",\n",
    ")\n",
    "\n",
    "# submit the command\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "# get a URL for the status of the job\n",
    "returned_job.studio_url"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read an MLTable in a job\n",
    "\n",
    "#### Create an environment\n",
    "\n",
    "Firstly, you need to create an environment that contains the mltable Python Library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "env_docker_conda = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "    conda_file=\"env-mltable.yml\",\n",
    "    name=\"mltable\",\n",
    "    description=\"Environment created for consuming MLTable.\",\n",
    ")\n",
    "\n",
    "ml_client.environments.create_or_update(env_docker_conda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "inputs = {\"input_data\": Input(type=AssetTypes.MLTABLE, path=\"./sample-mltable\")}\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",  # local path where the code is stored\n",
    "    command=\"python read_mltable.py --input_data ${{inputs.input_data}}\",\n",
    "    inputs=inputs,\n",
    "    environment=env_docker_conda,\n",
    "    compute=\"cpu-cluster\",\n",
    ")\n",
    "\n",
    "# submit the command\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "# get a URL for the status of the job\n",
    "returned_job.studio_url"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Read, write and register a data asset"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
